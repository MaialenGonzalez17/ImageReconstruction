# Image Reconstruction with Denoising Autoencoder

## General Description

This project implements a **Denosing Autoencoder** designed for **image restoration**. The model learns to reconstruct clean, high quality images from degraded versions affected by different types of distortions or artifacts. The methodology combines deep learning with dynamic data augmentation and weighted training to improve robustness against multiple degradations.

---

## What the Model Does

- **Input:** Degraded images generated by applying different synthetic transformations to original clean images.
- **Output:** Restored images that approximate the original images without degradation.
- **Objective:** Minimize reconstruction loss by learning to invert degradations, thus improving image quality in environments where the original captures are noisy or corrupted.

---

## Main Components

### 1. Dataset and Transformations

- Original images are loaded from a specified directory.
- Each training sample is augmented by applying a **random transformation** selected according to dynamic weights.
- These transformations simulate real artifacts, such as blur, noise, illumination variations, among others.
- The weights of these transformations are adaptively updated during training, focusing on those degradations that are more difficult for the model.

### 2. Model Architecture

- The base model is a **Convolutional Autoencoder** implemented in PyTorch.
- It uses encoder and decoder layers with **skip connections** (similar to U-Net architecture) to preserve spatial details.
- The encoder compresses the input image to a lower dimensional latent space.
- The decoder reconstructs the image from this compressed representation.

### 3. Loss Function

- A combined **MSE + SSIM** function is employed to optimize both per-pixel accuracy and perceptual similarity.
- The inclusion of SSIM helps preserve structure, textures and edges in the restored image.

### 4. Training Process

- The data set is divided into training and validation subsets.
- Training is run for a fixed number of epochs, dynamically updating the weights of the transformations according to the training loss.
- The model that performs best in validation is saved.
- Checkpoints are generated to allow training to continue from the last saved point.
- The loss history and the evolution of the transform weights are recorded.

---

## How to Use the Code

### Prerequisites

- Python 3.8 or higher
- PyTorch (tested with versions 1.10+)
- Additional libraries: `numpy`, `pandas`, `openpyxl` (for saving Excel files), and others used in the dataset and transformations modules.

### Running the Training

Before running, update the paths and parameters in the script:

```python
train(
    image_dir="/route/../",  # Folder with original clean images
    epochs=30,
    batch_size=16,
    learning_rate=1e-4,
    val_ratio=0.15,
    save_path="weights"  # Folder where models and records will be saved
)
````
# ü§ñ Metric Comparison: Degraded vs AI 

## üîç No-Reference Metrics (NR)

<div style="overflow-x: auto">

| Metric / Transformation       | D: Gauss | AI: Gauss | D: Noise | AI: Noise | D: Motion | AI: Motion | D: Light | AI: Light |
|------------------------------|----------|-----------|----------|-----------|-----------|------------|----------|-----------|
| Entropy                      | **7.21** | 7.07      | **7.64** | 6.97      | **7.10**  | 6.95       | **7.01** | 6.91      |
| Contrast (%)                 | **20.98**| 19.11     | **20.67**| 14.98     | **20.51** | 18.47      | **21.05**| 19.42     |
| Sharpness (%)                | 2.91     | **4.98**  | **100**  | **100**   | 13.85     | **15.10**  | **67.60**| 40.61     |
| Colorfulness (%)             | 44.35    | **45.78** | **54.32**| 37.07     | 45.82     | **46.00**  | 47.19    | **48.09** |
| BRISQUE                      | **0.72** | 0.70      | **0.72** | 0.43      | **0.51**  | 0.50       | **0.20** | 0.29      |
| NIQE                         | 0.64     | **0.57**  | **0.40** | 0.53      | 0.61      | **0.58**   | 0.63     | **0.59**  |
| NIMA                         | **2.33** | 2.25      | **6.34** | 3.19      | **2.41**  | 2.32       | **2.87** | 2.48      |

</div>

## üìä Reference-Based Metrics (RB)

<div style="overflow-x: auto">

| Metric / Transformation | D: Gauss | AI: Gauss | D: Noise | AI: Noise | D: Motion | AI: Motion | D: Light | AI: Light |
|-------------------------|----------|-----------|----------|-----------|-----------|------------|----------|-----------|
| PSNR                    | **35.81**| 30.97     | 28.12    | **28.50** | **36.82** | 30.82      | 29.61    | 29.61     |
| SSIM                    | **0.88** | **0.88**  | 0.14     | **0.60**  | **0.89**  | 0.88       | **0.93** | 0.92      |
| MSE                     | **18.91**| 53.71     | 100.34   | **92.93** | **16.05** | 56.27      | 84.23    | **76.76** |
| LPIPS                   | **0.16** | 0.15      | 0.75     | **0.39**  | **0.07**  | **0.07**   | **0.01** | 0.02      |

</div>
